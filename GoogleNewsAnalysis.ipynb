{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e6738c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading punkt: HTTP Error 403: Forbidden\n",
      "[nltk_data] Error loading stopwords: HTTP Error 403: Forbidden\n",
      "[nltk_data] Error loading wordnet: HTTP Error 403: Forbidden\n",
      "100%|██████████| 3005/3005 [00:02<00:00, 1465.25it/s]\n",
      "INFO:root:Computing O...\n",
      "INFO:root:Estimating \\mu...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            j Polarity  Coverage  Overlaps  Conflicts\n",
      "lf_contains_positive_words  0   [0, 1]       1.0       1.0   0.974376\n",
      "lf_contains_negative_words  1   [0, 1]       1.0       1.0   0.974376\n",
      "lf_heavy_positive_words     2   [0, 1]       1.0       1.0   0.974376\n",
      "lf_heavy_negative_words     3   [0, 1]       1.0       1.0   0.974376\n",
      "lf_predominant_sentiment    4   [0, 1]       1.0       1.0   0.974376\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/500 [00:00<?, ?epoch/s]INFO:root:[0 epochs]: TRAIN:[loss=13.884]\n",
      " 18%|█▊        | 92/500 [00:00<00:00, 913.56epoch/s]INFO:root:[100 epochs]: TRAIN:[loss=0.006]\n",
      " 37%|███▋      | 187/500 [00:00<00:00, 928.43epoch/s]INFO:root:[200 epochs]: TRAIN:[loss=0.003]\n",
      " 57%|█████▋    | 287/500 [00:00<00:00, 957.47epoch/s]INFO:root:[300 epochs]: TRAIN:[loss=0.002]\n",
      " 77%|███████▋  | 385/500 [00:00<00:00, 965.45epoch/s]INFO:root:[400 epochs]: TRAIN:[loss=0.002]\n",
      "100%|██████████| 500/500 [00:00<00:00, 966.18epoch/s]\n",
      "INFO:root:Finished Training\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mejor precisión: 0.8752079866888519\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.99      0.92       434\n",
      "           1       0.95      0.58      0.72       167\n",
      "\n",
      "    accuracy                           0.88       601\n",
      "   macro avg       0.91      0.78      0.82       601\n",
      "weighted avg       0.89      0.88      0.86       601\n",
      "\n",
      "Mejor modelo: SVC(C=1, kernel='linear', probability=True)\n",
      "\n",
      "Porcentaje de noticias negativas: 0.23%\n",
      "\n",
      "Análisis completado y resultados guardados en el archivo CSV.\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import time\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "from pygooglenews import GoogleNews\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from langdetect import detect\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from snorkel.labeling import labeling_function, PandasLFApplier, LFAnalysis\n",
    "from snorkel.labeling.model import LabelModel\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "# Descargar recursos de NLTK\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "\n",
    "# Preguntar al usuario por el nombre de la empresa y el año\n",
    "empresa = input(\"Introduce el nombre de la empresa a investigar: \")\n",
    "year = int(input(\"Introduce el año para la búsqueda de noticias (por ejemplo, 2023): \"))\n",
    "\n",
    "# Función para obtener títulos de noticias\n",
    "def get_titles(search, lang, from_date, to_date):\n",
    "    gn = GoogleNews(lang=lang)\n",
    "    stories = []\n",
    "    try:\n",
    "        search = gn.search(search, from_=from_date, to_=to_date)\n",
    "        newsitem = search['entries']\n",
    "        for item in newsitem:\n",
    "            story = {\n",
    "                'title': item.title,\n",
    "                'link': item.link,\n",
    "                'date': item.published\n",
    "            }\n",
    "            stories.append(story)\n",
    "    except Exception as e:\n",
    "        print(f\"Error al obtener noticias: {e}\")\n",
    "    return stories\n",
    "\n",
    "# Función para obtener títulos en diferentes intervalos y lenguajes en paralelo\n",
    "def get_all_titles(search, year):\n",
    "    all_stories = []\n",
    "    if year == datetime.now().year:\n",
    "        end_date = datetime.now()\n",
    "        start_date = end_date - timedelta(days=365)\n",
    "    else:\n",
    "        start_date = datetime(year, 1, 1)\n",
    "        end_date = datetime(year, 12, 31)\n",
    "\n",
    "    date_ranges = []\n",
    "    while start_date < end_date:\n",
    "        from_date = start_date.strftime('%m/%d/%Y')\n",
    "        to_date = (start_date + timedelta(days=30)).strftime('%m/%d/%Y')\n",
    "        date_ranges.append((from_date, to_date))\n",
    "        start_date += timedelta(days=30)\n",
    "\n",
    "    def fetch_news(lang):\n",
    "        stories = []\n",
    "        for date_range in date_ranges:\n",
    "            stories.extend(get_titles(search, lang, *date_range))\n",
    "            time.sleep(random.uniform(1, 5))\n",
    "        return stories\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=2) as executor:\n",
    "        results = list(executor.map(fetch_news, ['es', 'en']))\n",
    "\n",
    "    for result in results:\n",
    "        all_stories.extend(result)\n",
    "\n",
    "    return all_stories\n",
    "\n",
    "# Función para preprocesar el texto\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "\n",
    "    # Detectar idioma y configurar stopwords y lemmatizer\n",
    "    lang = 'spanish' if detect(text) == 'es' else 'english'\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    stop_words = set(stopwords.words(lang))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    processed_text = ' '.join(tokens)\n",
    "    return processed_text\n",
    "\n",
    "# Listas de palabras para etiquetado\n",
    "positive_words = [\"beneficio\", \"ganancia\", \"crecimiento\", \"seguro\", \"safe\", \"expansión\", \"mejora\", \"regalo\", \"gift\", \"logro\", \"logra\", \"dona\", \"otorga\", \"proporciona\", \"sostenible\", \"desarrollo\", \"amistoso\", \"saludable\", \"bondadoso\", \"benefit\", \"profit\", \"growth\", \"expansion\", \"improvement\", \"achieves\", \"donates\", \"grants\", \"provides\", \"sustainable\", \"development\", \"friendly\", \"healthy\", \"kind\", \"impulsa\", \"promote\", \"preserva\", \"preserve\", \"acuerdo\", \"celebra\", \"celebrate\", \"convivio\", \"reúne\", \"dialoga\", \"dialogue\", \"coopera\", \"cooperate\", \"concreta\", \"sustenta\", \"prolifera\", \"proliferate\", \"agreement\", \"excelente\", \"excellent\", \"perfect\", \"eficiente\", \"motiva\", \"motivate\", \"atiende\", \"serves\", \"éxito\", \"successful\", \"responsable\", \"concientiza\", \"raise\", \"apoyo\", \"awareness\", \"support\", \"resguarda\", \"inaugura\", \"protect\", \"inaugurate\", \"estrena\", \"debut\", \"repara\", \"repair\", \"ayuda\", \"helps\", \"reconoce\", \"recognize\", \"agradece\", \"thank\", \"creció\", \"ganancias\", \"iniciativa\", \"evolución\", \"evolution\", \"enjoy\", \"disfrutar\", \"gustar\", \"anuncia\", \"anounce\", \"recorta\", \"valioso\", \"valuable\", \"proteje\", \"resolve\", \"resuelve\", \"great\", \"atractivo\", \"compromiso\", \"commitment\", \"attractive\", \"prevención\", \"prevention\", \"solución\", \"solution\", \"invest\", \"invierte\", \"solar\", \"favorita\", \"favorite\", \"reparte\", \"instala\", \"install\", \"superior\", \"justo\", \"duplica\", \"duplicate\", \"ofrece\", \"offer\"]\n",
    "negative_words = [\"sospechoso\", \"despide\", \"fire\", \"desploma\", \"malware\", \"devolver\", \"trampa\", \"trap\", \"suplantación\", \"impersonation\", \"íntimas\", \"trolleo\", \"demanda\", \"filtra\", \"intimate\", \"protesta\", \"violencia\", \"agresivo\", \"peligroso\", \"bloqueo\", \"horrible\", \"contaminación\", \"exhibe\", \"violación\", \"guerra\", \"conflicto\", \"corruption\", \"suspect\", \"protest\", \"violence\", \"aggressive\", \"dangerous\", \"blockade\", \"horrible\", \"pollution\", \"exhibits\", \"war\", \"conflict\", \"condena\", \"sentence\", \"destroy\", \"destroza\", \"arrebato\", \"pelea\", \"pleito\", \"fight\", \"daño\", \"damage\", \"falla\", \"hackeo\", \"hacker\", \"hackean\", \"fault\", \"defect\", \"failure\", \"poison\", \"veneno\", \"ineficiente\", \"inútil\", \"inefficient\", \"useless\", \"vulnera\", \"accidente\", \"deficiente\", \"complain\", \"queja\", \"polémica\", \"polemic\", \"burlas\", \"mock\", \"teasing\", \"corte\", \"shortcut\", \"afectado\", \"affected\", \"cae\", \"fall\", \"obliga\", \"obligate\", \"pérdida\", \"loss\", \"vandaliza\", \"vandalize\", \"diagnosticado\", \"diagnosed\", \"riesgo\", \"risk\", \"problematic\", \"toxic\", \"murder\", \"asesinato\", \"cut\", \"radiation\", \"radiación\", \"esconde\", \"hide\", \"caos\", \"contra\", \"againts\", \"trabas\", \"disputa\", \"suspende\", \"mentira\", \"lie\", \"lier\", \"mentiroso\", \"engaño\", \"acortar\", \"abandona\", \"abandone\", \"víctima\", \"victim\", \"atentado\", \"attempt\", \"amenaza\", \"abusa\", \"abuse\", \"threat\", \"timo\", \"defectuoso\", \"faulty\", \"falsificar\", \"robar\", \"falsify\", \"steal\", \"infiel\", \"indigno\", \"infidel\", \"unworthy\", \"decepción\", \"alerta\", \"alert\", \"espiar\", \"spy\", \"spía\"]\n",
    "\n",
    "\n",
    "# Listas de palabras con peso cuádruple\n",
    "heavy_positive_words = [\"exitoso\", \"innovación\", \"innovation\", \"premio\", \"victoria\", \"award\", \"victory\", \"celebrate\", \"celebra\", \"desarrolla\", \"achievement\", \"beca\", \"scholarship\"]\n",
    "heavy_negative_words = [\"escándalo\", \"denuncia\", \"crisis\", \"delito\", \"quiebra\", \"abuso\", \"catastrófico\", \"fraude\", \"fraud\", \"crimen\", \"crime\", \"asesino\", \"muerto\", \"corrupción\", \"criminal\", \"intoxicación\", \"intoxication\", \"prisión\", \"encarcelado\", \"cárcel\", \"jail\", \"violación\", \"violation\", \"acusado\", \"leaked\", \"accused\", \"extorción\", \"extortion\", \"multa\", \"sanción\", \"sanction\", \"explosión\", \"explotion\",]\n",
    "\n",
    "@labeling_function()\n",
    "def lf_contains_positive_words(x):\n",
    "    return 0 if any(word in x['processed_title'] for word in positive_words) else 1\n",
    "\n",
    "@labeling_function()\n",
    "def lf_contains_negative_words(x):\n",
    "    return 1 if any(word in x['processed_title'] for word in negative_words) else 0\n",
    "\n",
    "@labeling_function()\n",
    "def lf_heavy_positive_words(x):\n",
    "    return 0 if any(word in x['processed_title'] for word in heavy_positive_words) else 1\n",
    "\n",
    "@labeling_function()\n",
    "def lf_heavy_negative_words(x):\n",
    "    return 1 if any(word in x['processed_title'] for word in heavy_negative_words) else 0\n",
    "\n",
    "@labeling_function()\n",
    "def lf_predominant_sentiment(x):\n",
    "    neg_count = sum(1 for word in x['processed_title'].split() if word in negative_words) + \\\n",
    "                4 * sum(1 for word in x['processed_title'].split() if word in heavy_negative_words)\n",
    "    pos_count = sum(1 for word in x['processed_title'].split() if word in positive_words) + \\\n",
    "                4 * sum(1 for word in x['processed_title'].split() if word in heavy_positive_words)\n",
    "    return 1 if neg_count > pos_count else 0\n",
    "\n",
    "# Cargar el conjunto de datos de noticias reales\n",
    "file_path = \"C:/Users/e-malandaf/Downloads/True6.csv\" \n",
    "df = pd.read_csv(file_path, encoding='latin1')\n",
    "\n",
    "# Limpiar la columna 'negativo'\n",
    "df['negativo'] = df['negativo'].replace('}', None)\n",
    "df['negativo'] = df['negativo'].dropna().astype(int)\n",
    "\n",
    "# Aplicar el preprocesamiento a la columna 'title'\n",
    "df['processed_title'] = df['title'].apply(preprocess_text)\n",
    "\n",
    "# Filtrar filas con valores no binarios en la columna 'negativo'\n",
    "df = df[df['negativo'].isin([0, 1])]\n",
    "\n",
    "# Eliminar filas donde 'processed_title' esté vacío\n",
    "df = df[df['processed_title'].str.strip().astype(bool)]\n",
    "\n",
    "# Crear una matriz TF-IDF para el texto procesado\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=1000, ngram_range=(1, 3), max_df=0.95, min_df=2)\n",
    "X_tfidf = tfidf_vectorizer.fit_transform(df['processed_title'])\n",
    "\n",
    "# Aplicar funciones de etiquetado a los datos\n",
    "lfs = [lf_contains_positive_words, lf_contains_negative_words, lf_heavy_positive_words, lf_heavy_negative_words, lf_predominant_sentiment]\n",
    "applier = PandasLFApplier(lfs)\n",
    "L_train = applier.apply(df)\n",
    "\n",
    "# Analizar las funciones de etiquetado\n",
    "lf_analysis = LFAnalysis(L=L_train, lfs=lfs).lf_summary()\n",
    "print(lf_analysis)\n",
    "\n",
    "# Modelado generativo para refinar las etiquetas\n",
    "label_model = LabelModel(cardinality=2, verbose=True)\n",
    "label_model.fit(L_train, n_epochs=500, log_freq=100, seed=42)\n",
    "\n",
    "# Obtener etiquetas refinadas\n",
    "df['snorkel_label'] = label_model.predict(L=L_train)\n",
    "\n",
    "# Utilizar las etiquetas refinadas por Snorkel como etiquetas de entrenamiento\n",
    "y_snorkel = df['snorkel_label'].astype(int)\n",
    "\n",
    "# Dividir los datos en conjuntos de entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_tfidf, y_snorkel, test_size=0.2, random_state=42)\n",
    "\n",
    "# Definir el modelo SVM\n",
    "svm_model = SVC(probability=True)\n",
    "\n",
    "# Definir los hiperparámetros a buscar\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10, 100],\n",
    "    'kernel': ['linear', 'rbf', 'poly'],\n",
    "    'gamma': ['scale', 'auto']\n",
    "}\n",
    "\n",
    "# Realizar la búsqueda de hiperparámetros\n",
    "grid_search = GridSearchCV(svm_model, param_grid, cv=5, scoring='accuracy')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Evaluar el mejor modelo encontrado\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n",
    "print(\"Mejor precisión:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "print(\"Mejor modelo:\", best_model)\n",
    "\n",
    "# Obtener todas las noticias sobre la empresa en el año especificado\n",
    "stories = get_all_titles(empresa, year)\n",
    "\n",
    "# Preprocesar y predecir el sentimiento de los títulos obtenidos, incluyendo probabilidad\n",
    "sospechosa_count = 0\n",
    "for story in stories:\n",
    "    processed_title = preprocess_text(story['title'])\n",
    "    title_tfidf = tfidf_vectorizer.transform([processed_title])\n",
    "    \n",
    "    # Obtener la predicción de sentimiento y la probabilidad\n",
    "    sentiment = best_model.predict(title_tfidf)[0]\n",
    "    probability = best_model.predict_proba(title_tfidf)[0][sentiment]\n",
    "    \n",
    "    # Asignar la etiqueta de sentimiento y la probabilidad al story\n",
    "    story['sentiment'] = 'Positive' if sentiment == 0 else 'Negative'\n",
    "    story['probability'] = probability\n",
    "    \n",
    "    # Contar las noticias sospechosas\n",
    "    if sentiment == 1: \n",
    "        sospechosa_count += 1\n",
    "\n",
    "# Calcular el porcentaje de noticias negativas solo si se encontraron historias\n",
    "if len(stories) > 0:\n",
    "    negative_percentage = (sospechosa_count / len(stories)) * 100\n",
    "    print(f\"\\nPorcentaje de noticias negativas: {negative_percentage:.2f}%\")\n",
    "else:\n",
    "    negative_percentage = 0\n",
    "    print(\"\\nNo se encontraron noticias para la empresa en el año especificado.\")\n",
    "\n",
    "# Guardar los resultados en un archivo CSV solo si hay historias\n",
    "if len(stories) > 0:\n",
    "    with open(f\"Resultados_{empresa}_{year}.csv\", \"w\", newline=\"\", encoding=\"utf-8-sig\") as csvfile:\n",
    "        fieldnames = [\"title\", \"link\", \"date\", \"sentiment\", \"probability\"]\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        for story in stories:\n",
    "            writer.writerow({\n",
    "                \"title\": story['title'],\n",
    "                \"link\": story['link'],\n",
    "                \"date\": story['date'],\n",
    "                \"sentiment\": story['sentiment'],\n",
    "                \"probability\": story['probability']\n",
    "            })\n",
    "    print(\"\\nAnálisis completado y resultados guardados en el archivo CSV.\")\n",
    "else:\n",
    "    print(\"\\nNo se generó ningún archivo CSV porque no se encontraron noticias.\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
